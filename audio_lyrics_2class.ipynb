{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(101) \n",
    "from transformers import AutoTokenizer,BertForSequenceClassification, BertTokenizer, DistilBertModel, AutoModelForSequenceClassification, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from accelerate import Accelerator\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=3001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files='lyrics_final.json')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "valence = []\n",
    "choi_rep = []\n",
    "sp_id = []\n",
    "seq_len = 256\n",
    "for i in tqdm(range(len(dataset['train']))):\n",
    "    input_encodings = tokenizer(str(dataset['train'][i][\"lyrics\"]),max_length=seq_len, \n",
    "                                truncation=True)\n",
    "    input_ids.append(torch.tensor(input_encodings['input_ids']))\n",
    "    attention_masks.append(torch.tensor(input_encodings['attention_mask']))\n",
    "    v_class = dataset['train'][i]['valence']\n",
    "    if v_class >= 5:\n",
    "        valence.append(1)\n",
    "    else:\n",
    "        valence.append(0)\n",
    "    id = dataset['train'][i]['id']\n",
    "    sp_id.append(id)\n",
    "    choi_path = os.path.join('D:/311511053/muse_music4all/choi_representaion',id+'.npy')\n",
    "    choi = np.load(choi_path)\n",
    "    choi = choi.flatten()\n",
    "    choi_rep.append(choi)\n",
    "choi_rep = torch.tensor(choi_rep)\n",
    "valence = torch.tensor(valence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_conv_layers, nums_feat_maps, feat_scale_factor,\n",
    "                 conv_sizes, pool_sizes, dropout_conv, input_shape,\n",
    "                 num_nin_layers=1, conv_until=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        if conv_until is None:\n",
    "            conv_until = num_conv_layers  # end-inclusive.\n",
    "        input_shape_specified = False\n",
    "        layers = []\n",
    "        for conv_idx in range(num_conv_layers):\n",
    "            n_feat_here = int(nums_feat_maps[conv_idx] * feat_scale_factor)\n",
    "            for _ in range(num_nin_layers):\n",
    "                if not input_shape_specified:\n",
    "                    layers.append(nn.Conv2d(input_shape[0], n_feat_here, kernel_size=conv_sizes[conv_idx], padding=(conv_sizes[conv_idx][0] // 2, conv_sizes[conv_idx][1] // 2)))\n",
    "                    input_shape_specified = True\n",
    "                else:\n",
    "                    layers.append(nn.Conv2d(n_feat_here, n_feat_here, kernel_size=conv_sizes[conv_idx], padding=(conv_sizes[conv_idx][0] // 2, conv_sizes[conv_idx][1] // 2)))\n",
    "                #layers.append(nn.BatchNorm2d(n_feat_here))\n",
    "                layers.append(nn.ELU(alpha=1.0))  # or choose your activation function\n",
    "\n",
    "            layers.append(nn.MaxPool2d(kernel_size=pool_sizes[conv_idx]))\n",
    "            if dropout_conv != 0.0:\n",
    "                layers.append(nn.Dropout(dropout_conv))\n",
    "            if conv_idx == conv_until:\n",
    "                break\n",
    "                \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "    def forward(self, mel):\n",
    "        x = self.conv_layers(mel)\n",
    "        x = self.pool(x)\n",
    "        #x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "        #x = x.flatten()\n",
    "        x = x.squeeze(2).squeeze(2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "\n",
    "args = [5,#num_conlayer\n",
    "            [32, 32, 32, 32, 32],#num_feat_map\n",
    "            1.0, #feat_scale_factor\n",
    "            [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "            poolings, #pool_sozes\n",
    "            0.0, #dropout_conv\n",
    "            (1,1,96,1360)]#intputshape\n",
    "model = ConvNet(*args, conv_until = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNet_weight = np.load('convNet_weight_5layers.npy',allow_pickle=True)\n",
    "convNet_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_state_dict = model.state_dict()\n",
    "for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "    # Convert weights from Keras to PyTorch format\n",
    "    if 'weight' in name:\n",
    "        #pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][i // 2][1]))\n",
    "        print(i, name, param.shape)\n",
    "    elif 'bias' in name:\n",
    "        #pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][i // 2][1]))\n",
    "        print(i, name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_state_dict = model.state_dict()\n",
    "count_c = 0\n",
    "count_b = 0\n",
    "for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "    # Convert weights from Keras to PyTorch format\n",
    "    if 'weight' in name and 'linear' not in name :\n",
    "        pytorch_state_dict[name] = torch.from_numpy(np.transpose(convNet_weight[1][0+count_c*6], (3,2,0,1)))\n",
    "        count_c += 1\n",
    "        print(i,name,pytorch_state_dict[name].shape)\n",
    "    elif 'bias' in name and 'linear' not in name :\n",
    "        pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][1+count_b*6]))\n",
    "        print(i,name,pytorch_state_dict[name].shape)\n",
    "        count_b += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id, label):\n",
    "      self.id = id\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "      mel = np.load('choi_mel/' + self.id[index] +'.npy') \n",
    "      mel = torch.tensor(mel.reshape(1, 96, 1360))\n",
    "      label = self.labels[index]\n",
    "      return mel, label\n",
    "    def __len__(self):\n",
    "      return len(self.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_convNet(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_convNet', train_batch_size = 8, learning_rate = 5e-5):\n",
    "    history = dict()\n",
    "    train_history_loss = []\n",
    "    train_history_acc = []\n",
    "    val_history_loss = []\n",
    "    val_history_acc = []\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    model = ConvNet(*args, conv_until = 5)\n",
    "    pytorch_state_dict = model.state_dict()\n",
    "    '''for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "        # Convert weights from Keras to PyTorch format\n",
    "        if 'weight' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.transpose(convNet_weight[1][0], (3,2,0,1)))\n",
    "        elif 'bias' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][i]))'''\n",
    "    pytorch_state_dict = model.state_dict()\n",
    "    count_c = 0\n",
    "    count_b = 0\n",
    "    for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "        # Convert weights from Keras to PyTorch format\n",
    "        if 'weight' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.transpose(convNet_weight[1][0+count_c*6], (3,2,0,1)))\n",
    "            count_c += 1\n",
    "        elif 'bias' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][1+count_b*6]))\n",
    "            count_b += 1\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            mel = batch[0].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            labels = batch[1].to(device)\n",
    "            outputs = model(\n",
    "                mel = mel\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        train_history_loss.append(average_loss)\n",
    "        train_history_acc.append(correct.item()/((len(train_dataloader) - 1) * train_batch_size + len(labels)))\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/((len(train_dataloader) - 1) * train_batch_size + len(labels)))\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    '''mel = torch.tensor([batch['choi']]).to(device)\n",
    "                    mel = mel.to(torch.float32)'''\n",
    "                    mel = batch[0].to(device)\n",
    "                    mel = mel.to(torch.float32)\n",
    "                    #labels = batch['labels'].clone().detach().to(device)\n",
    "                    labels = batch[1].to(device)\n",
    "                    output = model( \n",
    "                        mel = mel\n",
    "                    )\n",
    "                    loss = loss_function(output, labels.to(device))\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(output,1)\n",
    "                    correct += (predict_label==labels).sum()  \n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            val_history_loss.append(val_loss)\n",
    "            val_history_acc.append(val_accuracy)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                torch.save(model.conv_layers, save_file + '/fold_{}/model_best_loss1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/model_best_loss2'.format(fold) + '.pt')\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                #model.save_pretrained(save_file + '/fold_{}/model_best'.format(fold))\n",
    "                torch.save(model.conv_layers, save_file + '/fold_{}/model_best_acc1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/model_best_acc2'.format(fold) + '.pt')\n",
    "        else:\n",
    "            if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "        if epoch%5 == 4:\n",
    "            if not os.path.isdir(save_file):\n",
    "                os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            torch.save(model.conv_layers, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_1.pt')\n",
    "            torch.save(model.linear, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_2.pt')\n",
    "        if epoch < 30:\n",
    "            if not os.path.isdir(save_file):\n",
    "                os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            torch.save(model.conv_layers, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_1.pt')\n",
    "            torch.save(model.linear, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_2.pt')\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    torch.save(model.conv_layers, save_file + '/fold_{}/model_final1'.format(fold) + '.pt')\n",
    "    torch.save(model.linear, save_file + '/fold_{}/model_final2'.format(fold) + '.pt')\n",
    "    history['train_loss'] = train_history_loss\n",
    "    history['train_accuracy'] = train_history_acc\n",
    "    history['val_loss'] = val_history_loss\n",
    "    history['val_accuracy'] = val_history_acc\n",
    "    return history\n",
    "def test_convNet(test_dataloader,fold, load_best = 'loss', load_file = 'valence_convNet'):\n",
    "    print('testing')\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    if load_best == 'loss':\n",
    "        model = ConvNet(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_loss2\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = ConvNet(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_acc2\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = ConvNet(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_final2\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            mel = batch[0].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            #labels = torch.tensor(batch['labels'])\n",
    "            labels = batch[1].to(device)\n",
    "            output = model(\n",
    "                mel = mel\n",
    "            )\n",
    "            _,predict_label = torch.max(output,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "history = []\n",
    "batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(sp_id, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    #train_id = StandardScaler().fit_transform(id[train])\n",
    "    '''train_id = choi_rep[train]\n",
    "    scaler = StandardScaler().fit(train_id)\n",
    "    train_id = scaler.transform(train_id)'''\n",
    "    train_id, val_id, train_label, val_label = train_test_split(np.array(sp_id)[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(id=train_id, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(id=val_id, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    #test_choi = StandardScaler().fit_transform(id[test])\n",
    "    #test_id = scaler.transform(id[test])\n",
    "    test_id = np.array(sp_id)[test]\n",
    "    test_dataset = TrainDataset(id=test_id, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    his = train_convNet(train_dataloader, val_dataloader,i+1,num_epochs=20, train_batch_size = batch_size, learning_rate=5e-5)\n",
    "    history.append(his)\n",
    "    accuracy, f1, prec_recall, confusion_m = test_convNet(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,3))\n",
    "plt.subplot(151)\n",
    "plt.plot(history[0]['train_loss'])\n",
    "plt.plot(history[0]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(152)\n",
    "plt.plot(history[1]['train_loss'])\n",
    "plt.plot(history[1]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(153)\n",
    "plt.plot(history[2]['train_loss'])\n",
    "plt.plot(history[2]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(154)\n",
    "plt.plot(history[3]['train_loss'])\n",
    "plt.plot(history[3]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(155)\n",
    "plt.plot(history[4]['train_loss'])\n",
    "plt.plot(history[4]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,3))\n",
    "plt.subplot(151)\n",
    "plt.plot(history[0]['train_accuracy'])\n",
    "plt.plot(history[0]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(152)\n",
    "plt.plot(history[1]['train_accuracy'])\n",
    "plt.plot(history[1]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(153)\n",
    "plt.plot(history[2]['train_accuracy'])\n",
    "plt.plot(history[2]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(154)\n",
    "plt.plot(history[3]['train_accuracy'])\n",
    "plt.plot(history[3]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(155)\n",
    "plt.plot(history[4]['train_accuracy'])\n",
    "plt.plot(history[4]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(\"max_loss:\",np.max(history[i]['val_loss']),\"min_loss:\", np.min(history[i]['val_loss']), \"loss_diff:\",np.max(history[i]['val_loss'])-np.min(history[i]['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(np.argmin(history[i]['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_intermediate_output(nn.Module):\n",
    "    def __init__(self, num_conv_layers, nums_feat_maps, feat_scale_factor,\n",
    "                 conv_sizes, pool_sizes, dropout_conv, input_shape,\n",
    "                 num_nin_layers=1, conv_until=None):\n",
    "        super(get_intermediate_output, self).__init__()\n",
    "\n",
    "        if conv_until is None:\n",
    "            conv_until = num_conv_layers  # end-inclusive.\n",
    "        input_shape_specified = False\n",
    "        layers = []\n",
    "        for conv_idx in range(num_conv_layers):\n",
    "            n_feat_here = int(nums_feat_maps[conv_idx] * feat_scale_factor)\n",
    "            for _ in range(num_nin_layers):\n",
    "                if not input_shape_specified:\n",
    "                    layers.append(nn.Conv2d(input_shape[0], n_feat_here, kernel_size=conv_sizes[conv_idx], padding=(conv_sizes[conv_idx][0] // 2, conv_sizes[conv_idx][1] // 2)))\n",
    "                    input_shape_specified = True\n",
    "                else:\n",
    "                    layers.append(nn.Conv2d(n_feat_here, n_feat_here, kernel_size=conv_sizes[conv_idx], padding=(conv_sizes[conv_idx][0] // 2, conv_sizes[conv_idx][1] // 2)))\n",
    "                #layers.append(nn.BatchNorm2d(n_feat_here))\n",
    "                layers.append(nn.ELU(alpha=1.0))  # or choose your activation function\n",
    "\n",
    "            layers.append(nn.MaxPool2d(kernel_size=pool_sizes[conv_idx]))\n",
    "            if dropout_conv != 0.0:\n",
    "                layers.append(nn.Dropout(dropout_conv))\n",
    "            if conv_idx == conv_until:\n",
    "                break\n",
    "                \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "    def forward(self, mel):\n",
    "        x = self.conv_layers(mel)\n",
    "        x = self.pool(x)\n",
    "        #x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "        #x = x.flatten()\n",
    "        x = x.squeeze(2).squeeze(2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_output(test_dataloader,fold, load_best = 'loss', load_file = 'valence_convNet'):\n",
    "    print('testing')\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    if load_best == 'loss':\n",
    "        model = get_intermediate_output(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_loss2\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = get_intermediate_output(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_acc2\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = get_intermediate_output(*args, conv_until = 0).to(device)\n",
    "        model.conv_layers = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_final2\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    layer_output = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            mel = batch[0].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            #labels = torch.tensor(batch['labels'])\n",
    "            labels = batch[1].to(device)\n",
    "            output = model(\n",
    "                mel = mel\n",
    "            )\n",
    "            layer_output.append(output[0].cpu())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    \n",
    "    return layer_output, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(sp_id, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    if i != 4:\n",
    "        continue\n",
    "    test_id = np.array(sp_id)[test]\n",
    "    test_dataset = TrainDataset(id=test_id, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    layer_output, ans = intermediate_output(test_dataloader,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplayer_output = [t.numpy() for t in layer_output[0:1000]]\n",
    "nplayer_output = np.asarray(nplayer_output)\n",
    "nplayer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_conv_tsne = TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(nplayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = ans[0:1000]\n",
    "df[\"comp-1\"] = lyr_conv_tsne[:,0]\n",
    "df[\"comp-2\"] = lyr_conv_tsne[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df).set(title=\"convNet T-SNE projection\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = ans[0:1000]\n",
    "df[\"comp-1\"] = lyr_conv_tsne[:,0]\n",
    "df[\"comp-2\"] = lyr_conv_tsne[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df).set(title=\"convNet T-SNE projection\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Choi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, choi_rep, label):\n",
    "      self.choi_rep = choi_rep\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "      coala = self.choi_rep[index]    \n",
    "      label = self.labels[index]\n",
    "      return coala, label\n",
    "    def __len__(self):\n",
    "      return len(self.choi_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoiModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ChoiModel, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear2 = nn.Linear(32,2)\n",
    "    def forward(self, choi_rep):     \n",
    "        outputs = self.linear1(choi_rep)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear2(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_choi(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_choi', train_batch_size = 32, learning_rate = 5e-5):\n",
    "    history = dict()\n",
    "    train_history_loss = []\n",
    "    train_history_acc = []\n",
    "    val_history_loss = []\n",
    "    val_history_acc = []\n",
    "    model = ChoiModel()\n",
    "    '''for name, module in model.linear1.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight_regularizer = nn.L1L2(l1=1e-5, l2=1e-4)  # Equivalent to kernel_regularizer in Keras\n",
    "            module.bias_regularizer = nn.L2(1e-4)  # Equivalent to bias_regularizer in Keras\n",
    "            module.activity_regularizer = nn.L2(1e-5)  # Equivalent to activity_regularizer in Keras'''\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            choi_rep = batch[0].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "  \n",
    "            labels = batch[1].to(device)\n",
    "            outputs = model(\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        train_history_loss.append(average_loss)\n",
    "        train_history_acc.append(correct.item()/((len(train_dataloader) - 1) * train_batch_size + len(labels)))\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/((len(train_dataloader) - 1) * train_batch_size + len(labels)))\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    '''choi_rep = torch.tensor([batch['choi']]).to(device)\n",
    "                    choi_rep = choi_rep.to(torch.float32)'''\n",
    "                    choi_rep = batch[0].to(device)\n",
    "                    choi_rep = choi_rep.to(torch.float32)\n",
    "                    #labels = batch['labels'].clone().detach().to(device)\n",
    "                    labels = batch[1].to(device)\n",
    "                    output = model( \n",
    "                        choi_rep = choi_rep\n",
    "                    )\n",
    "                    loss = loss_function(output, labels.to(device))\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(output,1)\n",
    "                    correct += (predict_label==labels).sum()  \n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            val_history_loss.append(val_loss)\n",
    "            val_history_acc.append(val_accuracy)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                torch.save(model.linear1, save_file + '/fold_{}/model_best_loss1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear2, save_file + '/fold_{}/model_best_loss2'.format(fold) + '.pt')\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                #model.save_pretrained(save_file + '/fold_{}/model_best'.format(fold))\n",
    "                torch.save(model.linear1, save_file + '/fold_{}/model_best_acc1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear2, save_file + '/fold_{}/model_best_acc2'.format(fold) + '.pt')\n",
    "        else:\n",
    "            if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "        if epoch%5 == 4:\n",
    "            if not os.path.isdir(save_file):\n",
    "                os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            torch.save(model.linear1, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_1.pt')\n",
    "            torch.save(model.linear2, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_2.pt')\n",
    "        if epoch < 10:\n",
    "            if not os.path.isdir(save_file):\n",
    "                os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            torch.save(model.linear1, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_1.pt')\n",
    "            torch.save(model.linear2, save_file + '/fold_{}/model_epoch_'.format(fold)+str(epoch+1)+'_2.pt')\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    torch.save(model.linear1, save_file + '/fold_{}/model_final1'.format(fold) + '.pt')\n",
    "    torch.save(model.linear2, save_file + '/fold_{}/model_final2'.format(fold) + '.pt')\n",
    "    history['train_loss'] = train_history_loss\n",
    "    history['train_accuracy'] = train_history_acc\n",
    "    history['val_loss'] = val_history_loss\n",
    "    history['val_accuracy'] = val_history_acc\n",
    "    return history\n",
    "def test_choi(test_dataloader,fold, load_best = 'loss', load_file = 'valence_choi'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        model = ChoiModel().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        model.linear2 = torch.load(load_file + \"/fold_{}/model_best_loss2\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = ChoiModel().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        model.linear2 = torch.load(load_file + \"/fold_{}/model_best_acc2\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = ChoiModel().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        model.linear2 = torch.load(load_file + \"/fold_{}/model_final2\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            choi_rep = batch[0].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            #labels = torch.tensor(batch['labels'])\n",
    "            labels = batch[1].to(device)\n",
    "            output = model(\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            _,predict_label = torch.max(output,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "history = []\n",
    "batch_size = 64\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(choi_rep, valence))):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    #train_chois = StandardScaler().fit_transform(choi_rep[train])\n",
    "    train_chois = choi_rep[train]\n",
    "    scaler = StandardScaler().fit(train_chois)\n",
    "    train_chois = scaler.transform(train_chois)\n",
    "    train_chois, val_coala, train_label, val_label = train_test_split(train_chois, valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(choi_rep=train_chois, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(choi_rep=val_coala, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    #test_choi = StandardScaler().fit_transform(choi_rep[test])\n",
    "    test_chois = scaler.transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(choi_rep=test_chois, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    his = train_choi(train_dataloader, val_dataloader,i+1,num_epochs=200, train_batch_size = batch_size, learning_rate=5e-4)\n",
    "    history.append(his)\n",
    "    accuracy, f1, prec_recall, confusion_m = test_choi(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "history = []\n",
    "batch_size = 64\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    #train_chois = StandardScaler().fit_transform(choi_rep[train])\n",
    "    train_chois = choi_rep[train]\n",
    "    scaler = StandardScaler().fit(train_chois)\n",
    "    train_chois = scaler.transform(train_chois)\n",
    "    train_chois, val_coala, train_label, val_label = train_test_split(train_chois, valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(choi_rep=train_chois, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(choi_rep=val_coala, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    #test_choi = StandardScaler().fit_transform(choi_rep[test])\n",
    "    test_chois = scaler.transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(choi_rep=test_chois, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    his = train_choi(train_dataloader, val_dataloader,i+1,num_epochs=50, train_batch_size = batch_size, learning_rate=5e-4)\n",
    "    history.append(his)\n",
    "    accuracy, f1, prec_recall, confusion_m = test_choi(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,3))\n",
    "plt.subplot(151)\n",
    "plt.plot(history[0]['train_loss'])\n",
    "plt.plot(history[0]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(152)\n",
    "plt.plot(history[1]['train_loss'])\n",
    "plt.plot(history[1]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(153)\n",
    "plt.plot(history[2]['train_loss'])\n",
    "plt.plot(history[2]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(154)\n",
    "plt.plot(history[3]['train_loss'])\n",
    "plt.plot(history[3]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(155)\n",
    "plt.plot(history[4]['train_loss'])\n",
    "plt.plot(history[4]['val_loss'])\n",
    "plt.title('loss history')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,3))\n",
    "plt.subplot(151)\n",
    "plt.plot(history[0]['train_accuracy'])\n",
    "plt.plot(history[0]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(152)\n",
    "plt.plot(history[1]['train_accuracy'])\n",
    "plt.plot(history[1]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(153)\n",
    "plt.plot(history[2]['train_accuracy'])\n",
    "plt.plot(history[2]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(154)\n",
    "plt.plot(history[3]['train_accuracy'])\n",
    "plt.plot(history[3]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(155)\n",
    "plt.plot(history[4]['train_accuracy'])\n",
    "plt.plot(history[4]['val_accuracy'])\n",
    "plt.title('accuracy history')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(\"max_loss:\",np.max(history[i]['val_loss']),\"min_loss:\", np.min(history[i]['val_loss']), \"loss_diff:\",np.max(history[i]['val_loss'])-np.min(history[i]['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(np.argmin(history[i]['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_start_ep = [125, 190, 190, 100, 175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(history[i]['val_loss'][choi_start_ep[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(np.max(history[i]['val_loss']) - 0.7 * (np.max(history[i]['val_loss'])-np.min(history[i]['val_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[0]['val_loss'][35], history[1]['val_loss'][30], history[2]['val_loss'][35] ,history[3]['val_loss'][30], history[4]['val_loss'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_start_ep = [35, 30, 35, 30, 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"fold {}\".format(i+1))\n",
    "    print(\"max_accuracy:\",np.max(history[i]['val_accuracy']),\"min_accuracy:\", np.min(history[i]['val_accuracy']), \"accuracy_diff:\",np.max(history[i]['val_accuracy'])-np.min(history[i]['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_intermediate_output(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(get_intermediate_output, self).__init__()\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear2 = nn.Linear(32,2)\n",
    "    def forward(self, choi_rep):     \n",
    "        outputs = self.linear1(choi_rep)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_output(test_dataloader,fold, load_best = 'loss', load_file = 'valence_choi'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        model = get_intermediate_output().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        #model.linear2 = torch.load(load_file + \"/fold_{}/model_best_loss2\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = get_intermediate_output().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        #model.linear2 = torch.load(load_file + \"/fold_{}/model_best_acc2\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = get_intermediate_output().to(device)\n",
    "        model.linear1 = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        #model.linear2 = torch.load(load_file + \"/fold_{}/model_final2\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    layer_output = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    #alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            choi_rep = batch[0].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            labels = batch[1].to(device)\n",
    "            outputs = model(\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            layer_output.append(outputs[0].cpu())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    \n",
    "    return layer_output, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    if i != 2:\n",
    "        continue\n",
    "    #train_chois = StandardScaler().fit_transform(choi_rep[train])\n",
    "    scaler = StandardScaler().fit(choi_rep[train])\n",
    "    train_chois = scaler.transform(choi_rep[train])\n",
    "    train_chois, val_coala, train_label, val_label = train_test_split(train_chois,  valence[train], test_size=0.1, random_state=42)\n",
    "\n",
    "    test_chois = scaler.transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(choi_rep=test_chois, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "    layer_output, ans = intermediate_output(test_dataloader,i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplayer_output = [t.numpy() for t in layer_output[0:1000]]\n",
    "nplayer_output = np.asarray(nplayer_output)\n",
    "nplayer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_conv_tsne = TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(nplayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = ans[0:1000]\n",
    "df[\"comp-1\"] = lyr_conv_tsne[:,0]\n",
    "df[\"comp-2\"] = lyr_conv_tsne[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df).set(title=\"ConvNet T-SNE projection\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosModel(nn.Module):\n",
    "    def __init__(self,pretrain_weight):\n",
    "        super(PosModel, self).__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(pretrain_weight)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(28, 2) # output features from bert is 768 and 2 is ur number of labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, label):\n",
    "      self.input_ids = input_ids\n",
    "      self.attention_masks = attention_masks\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "      input_id = self.input_ids[index]\n",
    "      attention_mask = self.attention_masks[index]\n",
    "      label = self.labels[index]\n",
    "      return input_id, attention_mask, label\n",
    "    def __len__(self):\n",
    "      return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lyrics(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_lyrics', learning_rate = 5e-5):\n",
    "    model = PosModel('SamLowe/roberta-base-go_emotions')\n",
    "    accelerator = Accelerator()\n",
    "    model  = accelerator.prepare(model)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #model.train()\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            #print(input_ids.dtype,attention_mask.dtype, coala_rep.dtype)\n",
    "            labels = batch[2].to(device)\n",
    "            #print(labels)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            #print(outputs.logits)\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "            \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/len(train_dataloader))\n",
    "        #model.eval()\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            #loss_function = torch.nn.CrossEntropyLoss()\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    input_ids =  batch[0].to(device)\n",
    "                    attention_mask =  batch[1].to(device)\n",
    "                    labels = batch[2].to(device)\n",
    "                    output = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    loss = loss_function(output, labels.to(device))\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(output,1)\n",
    "                    correct += (predict_label==labels).sum()\n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                model.base_model.save_pretrained(save_file + '/fold_{}/model_best_loss'.format(fold))\n",
    "                torch.save(model.linear, save_file + '/fold_{}/model_best_loss1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_loss'.format(fold))\n",
    "            #else:\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                #model.save_pretrained(save_file + '/fold_{}/model_best'.format(fold))\n",
    "                model.base_model.save_pretrained(save_file + '/fold_{}/model_best_acc'.format(fold))\n",
    "                torch.save(model.linear, save_file + '/fold_{}/model_best_acc1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_acc'.format(fold))\n",
    "        #else:\n",
    "        if not os.path.isdir(save_file):\n",
    "                os.mkdir(save_file)\n",
    "        if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "            os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "        model.base_model.save_pretrained(save_file + '/fold_{}/model'.format(fold) + str(epoch+1))\n",
    "        torch.save(model.linear, save_file + '/fold_{}/model_'.format(fold) + str(epoch+1) + '.pt')\n",
    "        tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_'.format(fold) + str(epoch+1))\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    #model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    model.base_model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    torch.save(model.linear, save_file + '/fold_{}/model_final1'.format(fold) + '.pt')\n",
    "    tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_final'.format(fold))\n",
    "def test_lyrics(test_dataloader,fold, load_best = 'loss', load_file = 'valence_lyrics'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        model = PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = PosModel(load_file + \"/fold_{}/model_final\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            #outputs = model(**data)\n",
    "            _,predict_label = torch.max(output,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            #summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            #print(summary_text)\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(np.array(input_ids), valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    train_coalas = StandardScaler().fit_transform(choi_rep[train])\n",
    "    train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(np.array(input_ids)[train], np.array(attention_masks)[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    test_coalas = StandardScaler().fit_transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids)[test], attention_masks = np.array(attention_masks)[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_lyrics(train_dataloader, val_dataloader,i+1,num_epochs=5, learning_rate=5e-7)\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_lyrics(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_intermediate_output(nn.Module):\n",
    "    def __init__(self,pretrain_weight):\n",
    "        super(get_intermediate_output, self).__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(pretrain_weight)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(28, 2) # output features from bert is 768 and 2 is ur number of labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        #outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_output(test_dataloader,fold, load_best = 'loss', load_file = 'valence_lyrics'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        model = get_intermediate_output(load_file + \"/fold_{}/model_best_loss\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = get_intermediate_output(load_file + \"/fold_{}/model_best_acc\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        model = get_intermediate_output(load_file + \"/fold_{}/model_final\".format(fold)).to(device)\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "    model.eval()\n",
    "    layer_output = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            layer_output.append(output[0].cpu())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "\n",
    "    return layer_output, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(np.array(input_ids,dtype=\"object\"), valence))):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids,dtype=\"object\")[test], attention_masks = np.array(attention_masks,dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "        \n",
    "    layer_output, ans = intermediate_output(test_dataloader,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplayer_output = [t.numpy() for t in layer_output[0:1000]]\n",
    "nplayer_output = np.asarray(nplayer_output)\n",
    "nplayer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_conv_tsne = TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(nplayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = ans[0:1000]\n",
    "df[\"comp-1\"] = lyr_conv_tsne[:,0]\n",
    "df[\"comp-2\"] = lyr_conv_tsne[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df).set(title=\"BERT T-SNE projection\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Choi + Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosModel(nn.Module):\n",
    "    def __init__(self,pretrain_weight):\n",
    "        super(PosModel, self).__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(pretrain_weight)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(28, 2) # output features from bert is 768 and 2 is ur number of labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, choi_rep, label):\n",
    "      self.input_ids = input_ids\n",
    "      self.attention_masks = attention_masks\n",
    "      self.choi_rep = choi_rep\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "      input_id = self.input_ids[index]\n",
    "      attention_mask = self.attention_masks[index]\n",
    "      coala = self.choi_rep[index]    \n",
    "      label = self.labels[index]\n",
    "      return input_id, attention_mask, coala, label\n",
    "    def __len__(self):\n",
    "      return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_fusion', learning_rate_lyrics = 5e-7, learning_rate_choi = 5e-5, choi_start_ep = 20):\n",
    "    lyrics_model = PosModel('SamLowe/roberta-base-go_emotions')\n",
    "    #model = PosModel(\"arousal_lyrics/fold_{}/model_best_loss\".format(fold))\n",
    "    accelerator = Accelerator()\n",
    "    lyrics_model  = accelerator.prepare(lyrics_model)\n",
    "    lyrics_optimizer = torch.optim.AdamW(lyrics_model.parameters(), lr=learning_rate_lyrics)\n",
    "    choi_model = ChoiModel()\n",
    "    '''choi_model.linear1 = torch.load('arousal_coala/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_1.pt')\n",
    "    choi_model.linear2 = torch.load('arousal_coala/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_2.pt')'''\n",
    "    choi_model.linear1 = torch.load('valence_choi/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_1.pt')\n",
    "    choi_model.linear2 = torch.load('valence_choi/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_2.pt')\n",
    "    choi_optimizer = torch.optim.AdamW(choi_model.parameters(), lr=learning_rate_choi)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #num_epochs = 10  # Set the number of training epochs\n",
    "    alpha = 0.5\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #model.train()\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            \n",
    "            lyrics_model.train()\n",
    "            choi_model.train()\n",
    "            #with torch.no_grad():\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            choi_rep = batch[2].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            #print(input_ids.dtype,attention_mask.dtype, choi_rep.dtype)\n",
    "            labels = batch[3].to(device)\n",
    "            #print(labels)\n",
    "            outputs1 = lyrics_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            outputs2 = choi_model( \n",
    "                        choi_rep = choi_rep\n",
    "                    )\n",
    "            outputs =(1-alpha) * outputs1 + alpha * outputs2 \n",
    "            loss = loss_function(outputs, labels)\n",
    "            lyrics_optimizer.zero_grad()\n",
    "            choi_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            lyrics_optimizer.step()\n",
    "            choi_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            #print(outputs.logits)\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            \n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "            \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/len(train_dataloader))\n",
    "        #model.eval()\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            lyrics_model.eval()\n",
    "            choi_model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            #loss_function = torch.nn.CrossEntropyLoss()\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    input_ids =  batch[0].to(device)\n",
    "                    attention_mask =  batch[1].to(device)\n",
    "                    choi_rep = batch[2].to(device)\n",
    "                    choi_rep = choi_rep.to(torch.float32)\n",
    "                    labels = batch[3].to(device)\n",
    "                    outputs1 = lyrics_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    outputs2 = choi_model( \n",
    "                                choi_rep = choi_rep\n",
    "                            )\n",
    "                    outputs =(1-alpha) * outputs1 + alpha * outputs2 \n",
    "                    loss = loss_function(outputs, labels.to(device))\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(outputs,1)\n",
    "                    correct += (predict_label==labels).sum()\n",
    "                    \n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                lyrics_model.base_model.save_pretrained(save_file + '/fold_{}/model_best_loss'.format(fold))\n",
    "                torch.save(lyrics_model.linear, save_file + '/fold_{}/model_best_loss1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_loss'.format(fold))\n",
    "                torch.save(choi_model.linear1, save_file + '/fold_{}/choi_model_best_loss1'.format(fold) + '.pt')\n",
    "                torch.save(choi_model.linear2, save_file + '/fold_{}/choi_model_best_loss2'.format(fold) + '.pt')\n",
    "            #else:\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                #model.save_pretrained(save_file + '/fold_{}/model_best'.format(fold))\n",
    "                lyrics_model.base_model.save_pretrained(save_file + '/fold_{}/model_best_acc'.format(fold))\n",
    "                torch.save(lyrics_model.linear, save_file + '/fold_{}/model_best_acc1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_acc'.format(fold))\n",
    "                torch.save(choi_model.linear1, save_file + '/fold_{}/choi_model_best_acc1'.format(fold) + '.pt')\n",
    "                torch.save(choi_model.linear2, save_file + '/fold_{}/choi_model_best_acc2'.format(fold) + '.pt')\n",
    "        else:\n",
    "            if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            lyrics_model.save_pretrained(save_file + '/fold_{}/model'.format(fold) + str(epoch+1))\n",
    "            tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer'.format(fold) + str(epoch+1))\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    #model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    lyrics_model.base_model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    torch.save(lyrics_model.linear, save_file + '/fold_{}/model_final1'.format(fold) + '.pt')\n",
    "    tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_final'.format(fold))\n",
    "    torch.save(choi_model.linear1, save_file + '/fold_{}/choi_model_final1'.format(fold) + '.pt')\n",
    "    torch.save(choi_model.linear2, save_file + '/fold_{}/choi_model_final2'.format(fold) + '.pt')\n",
    "def test_fusion(test_dataloader,fold, load_best = 'loss', load_file = 'valence_fusion'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        lyrics_model = PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)).to(device)\n",
    "        lyrics_model.linear = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        choi_model = ChoiModel()\n",
    "        choi_model.linear1 = torch.load(load_file + \"/fold_{}/choi_model_best_loss1\".format(fold)+ '.pt')\n",
    "        choi_model.linear2 = torch.load(load_file + \"/fold_{}/choi_model_best_loss2\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        lyrics_model = PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)).to(device)\n",
    "        lyrics_model.linear = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        choi_model = ChoiModel()\n",
    "        choi_model.linear1 = torch.load(load_file + \"/fold_{}/choi_model_best_acc1\".format(fold)+ '.pt')\n",
    "        choi_model.linear2 = torch.load(load_file + \"/fold_{}/choi_model_best_acc2\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        #model = BertForSequenceClassification.from_pretrained(load_file + \"/fold_{}/model5\".format(fold)).to(device)\n",
    "        lyrics_model = PosModel(load_file + \"/fold_{}/model_final\".format(fold)).to(device)\n",
    "        lyrics_model.linear = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        choi_model = ChoiModel()\n",
    "        choi_model.linear1 = torch.load(load_file + \"/fold_{}/choi_model_final1\".format(fold)+ '.pt')\n",
    "        choi_model.linear2 = torch.load(load_file + \"/fold_{}/choi_model_final2\".format(fold)+ '.pt')\n",
    "    lyrics_model.eval()\n",
    "    choi_model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            choi_rep = batch[2].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            outputs1 = lyrics_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            outputs2 = choi_model( \n",
    "                        choi_rep = choi_rep\n",
    "                    )\n",
    "            outputs =(1-alpha) * outputs1 + alpha * outputs2 \n",
    "            #outputs = model(**data)\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            #summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            #print(summary_text)\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    #train_chois = StandardScaler().fit_transform(choi_rep[train])\n",
    "    train_chois = choi_rep[train]\n",
    "    scaler = StandardScaler().fit(train_chois)\n",
    "    train_chois = scaler.transform(train_chois)\n",
    "    train_chois, val_coala, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(train_chois, np.array(input_ids)[train], np.array(attention_masks)[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, choi_rep=train_chois, label = train_label)\n",
    "    \n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, choi_rep=val_coala, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    #test_chois = StandardScaler().fit_transform(choi_rep[test])\n",
    "    test_chois = scaler.transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids)[test], attention_masks = np.array(attention_masks)[test], choi_rep=test_chois, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion(train_dataloader, val_dataloader,i+1,num_epochs=10, learning_rate_lyrics=5e-7, learning_rate_choi = 5e-4, choi_start_ep=choi_start_ep[i], save_file='valence_fusion')\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion(test_dataloader,i+1, load_file='valence_fusion')\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain epoch 30\n",
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concate fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, choi_rep, label):\n",
    "      self.input_ids = input_ids\n",
    "      self.attention_masks = attention_masks\n",
    "      self.choi_rep = choi_rep\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "      input_id = self.input_ids[index]\n",
    "      attention_mask = self.attention_masks[index]\n",
    "      coala = self.choi_rep[index]    \n",
    "      label = self.labels[index]\n",
    "      return input_id, attention_mask, coala, label\n",
    "    def __len__(self):\n",
    "      return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class PosModel(nn.Module):\n",
    "    def __init__(self,pretrain_weight = 'SamLowe/roberta-base-go_emotions'):\n",
    "        super(PosModel, self).__init__()\n",
    "        \n",
    "        #self.base_model = AutoModelForSequenceClassification.from_pretrained(pretrain_weight).roberta\n",
    "        self.base_model = RobertaForSequenceClassification.from_pretrained('SamLowe/roberta-base-go_emotions', num_labels=128, ignore_mismatched_sizes=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        #self.linar = nn.Linear(256*768,768)\n",
    "        self.linear = nn.Linear(128, 2) # output features from bert is 768 and 2 is ur number of labels\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        #outputs = outputs[0].flatten()\n",
    "        # You write you new head here\n",
    "        #outputs = self.linar(outputs)\n",
    "        #outputs = self.dropout(outputs[0])\n",
    "        #outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.lyric = model1\n",
    "        self.choi = model2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(60,32),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, choi_rep):\n",
    "        lyric_outputs = self.lyric.base_model(input_ids, attention_mask=attention_mask)\n",
    "        #lyric_outputs = self.dropout(lyric_outputs[0])    #output:28\n",
    "        choi_outputs = self.choi.linear1(choi_rep)              #output:32  \n",
    "        #print(choi_outputs.shape, lyric_outputs.shape)\n",
    "        outputs = torch.cat((lyric_outputs[0], choi_outputs),1)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear(outputs)                  #output:2\n",
    "        '''choi_outputs = self.choi.linear2(choi_outputs) #output:2\n",
    "        lyric_outputs = self.lyric.linear(lyric_outputs) #output:2\n",
    "        outputs = self.alpha * outputs + (1-self.alpha) * choi_outputs\n",
    "        return choi_outputs, lyric_outputs, outputs'''\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_2(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_fusion', learning_rate_lyrics = 5e-7, learning_rate_coala = 5e-5, choi_start_ep = 20):\n",
    "    model = HybridModel(PosModel('SamLowe/roberta-base-go_emotions'),ChoiModel()).to(device)\n",
    "    model.choi.linear1 = torch.load('valence_choi/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_1.pt')\n",
    "    model.choi.linear2 = torch.load('valence_choi/fold_{}/model_epoch_'.format(fold)+str(choi_start_ep)+'_2.pt')\n",
    "    '''model = HybridModel(PosModel(\"valence_lyrics/fold_{}/model_best_loss\".format(fold)),ChoiModel()).to(device)\n",
    "    model.choi.linear1 = torch.load('valence_choi/fold_{}/model_best_loss'.format(fold)+'1.pt')\n",
    "    model.choi.linear2 = torch.load('valence_choi/fold_{}/model_best_loss'.format(fold)+'2.pt')'''\n",
    "    lyrics_optimizer = torch.optim.AdamW(model.lyric.base_model.parameters(), lr=learning_rate_lyrics)\n",
    "    coala_optimizer = torch.optim.AdamW(model.choi.linear1.parameters(), lr=learning_rate_coala)\n",
    "    hybrid_optimizer = torch.optim.AdamW(model.linear.parameters(), lr=learning_rate_coala)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #num_epochs = 10  # Set the number of training epochs\n",
    "    alpha = 0.5\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #model.train()\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            #with torch.no_grad():\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            choi_rep = batch[2].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            #print(input_ids.dtype,attention_mask.dtype, choi_rep.dtype)\n",
    "            labels = batch[3].to(device)\n",
    "            #print(labels)\n",
    "            '''coala_output, lyrics_output, hybrid_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            outputs = (1/3) * coala_output + (1/3) * lyrics_output + (1/3) * hybrid_outputs '''\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            lyrics_optimizer.zero_grad()\n",
    "            coala_optimizer.zero_grad()\n",
    "            hybrid_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            lyrics_optimizer.step()\n",
    "            coala_optimizer.step()\n",
    "            hybrid_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            #print(outputs.logits)\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            \n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "            \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/len(train_dataloader))\n",
    "        #model.eval()\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            #loss_function = torch.nn.CrossEntropyLoss()\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    input_ids =  batch[0].to(device)\n",
    "                    attention_mask =  batch[1].to(device)\n",
    "                    choi_rep = batch[2].to(device)\n",
    "                    choi_rep = choi_rep.to(torch.float32)\n",
    "                    labels = batch[3].to(device)\n",
    "                    '''coala_output, lyrics_output, hybrid_outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        choi_rep = choi_rep\n",
    "                    )\n",
    "                    \n",
    "                    outputs = (1/3) * coala_output + (1/3) * lyrics_output + (1/3) * hybrid_outputs '''\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        choi_rep = choi_rep\n",
    "                    )\n",
    "                    loss = loss_function(outputs, labels.to(device))\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(outputs,1)\n",
    "                    correct += (predict_label==labels).sum()\n",
    "                    \n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_best_loss'.format(fold))\n",
    "                #torch.save(model.lyric.linear, save_file + '/fold_{}/model_best_loss1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_loss'.format(fold))\n",
    "                torch.save(model.choi.linear1, save_file + '/fold_{}/coala_model_best_loss1'.format(fold) + '.pt')\n",
    "                torch.save(model.choi.linear2, save_file + '/fold_{}/coala_model_best_loss2'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/hybrid_model_best_loss'.format(fold) + '.pt')\n",
    "            #else:\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                #model.save_pretrained(save_file + '/fold_{}/model_best'.format(fold))\n",
    "                model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_best_acc'.format(fold))\n",
    "                #torch.save(model.lyric.linear, save_file + '/fold_{}/model_best_acc1'.format(fold) + '.pt')\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_acc'.format(fold))\n",
    "                torch.save(model.choi.linear1, save_file + '/fold_{}/coala_model_best_acc1'.format(fold) + '.pt')\n",
    "                torch.save(model.choi.linear2, save_file + '/fold_{}/coala_model_best_acc2'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/hybrid_model_best_acc'.format(fold) + '.pt')\n",
    "        else:\n",
    "            if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            model.lyric.save_pretrained(save_file + '/fold_{}/model'.format(fold) + str(epoch+1))\n",
    "            tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer'.format(fold) + str(epoch+1))\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    #model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    #torch.save(model.lyric.linear, save_file + '/fold_{}/model_final1'.format(fold) + '.pt')\n",
    "    tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_final'.format(fold))\n",
    "    torch.save(model.choi.linear1, save_file + '/fold_{}/coala_model_final1'.format(fold) + '.pt')\n",
    "    torch.save(model.choi.linear2, save_file + '/fold_{}/coala_model_final2'.format(fold) + '.pt')\n",
    "    torch.save(model.linear, save_file + '/fold_{}/hybrid_model_final'.format(fold) + '.pt')\n",
    "    \n",
    "def test_fusion_2(test_dataloader,fold, load_best = 'loss', load_file = 'arousal_fusion'):\n",
    "    print('testing')\n",
    "    if load_best == 'loss':\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)),ChoiModel()).to(device)\n",
    "        #lyrics_model = PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)).to(device)\n",
    "        #model.lyric.linear = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        #coala_model = ChoiModel()\n",
    "        model.choi.linear1 = torch.load(load_file + \"/fold_{}/coala_model_best_loss1\".format(fold)+ '.pt')\n",
    "        model.choi.linear2 = torch.load(load_file + \"/fold_{}/coala_model_best_loss2\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_loss\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)),ChoiModel()).to(device)\n",
    "        #lyrics_model = PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)).to(device)\n",
    "        #model.lyric.linear = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        #coala_model = ChoiModel()\n",
    "        model.choi.linear1 = torch.load(load_file + \"/fold_{}/coala_model_best_acc1\".format(fold)+ '.pt')\n",
    "        model.choi.linear2 = torch.load(load_file + \"/fold_{}/coala_model_best_acc2\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_acc\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_final\".format(fold)),ChoiModel()).to(device)\n",
    "        #lyrics_model = PosModel(load_file + \"/fold_{}/model_final\".format(fold)).to(device)\n",
    "        #model.lyric.linear = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        #coala_model = ChoiModel()\n",
    "        model.choi.linear1 = torch.load(load_file + \"/fold_{}/coala_model_final1\".format(fold)+ '.pt')\n",
    "        model.choi.linear2 = torch.load(load_file + \"/fold_{}/coala_model_final2\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_final\".format(fold)+ '.pt')\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    #alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            choi_rep = batch[2].to(device)\n",
    "            choi_rep = choi_rep.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            '''coala_output, lyrics_output, hybrid_outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            outputs = (1/3) * coala_output + (1/3) * lyrics_output + (1/3) * hybrid_outputs '''\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                choi_rep = choi_rep\n",
    "            )\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            #summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            #print(summary_text)\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    #train_chois = StandardScaler().fit_transform(choi_rep[train])\n",
    "    scaler = StandardScaler().fit(choi_rep[train])\n",
    "    train_chois = scaler.transform(choi_rep[train])\n",
    "    train_chois, val_coala, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(train_chois, np.array(input_ids)[train], np.array(attention_masks)[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, choi_rep=train_chois, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, choi_rep=val_coala, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    #test_chois = StandardScaler().fit_transform(choi_rep[test])\n",
    "    test_chois = scaler.transform(choi_rep[test])\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids)[test], attention_masks = np.array(attention_masks)[test], choi_rep=test_chois, label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion_2(train_dataloader, val_dataloader,i+1,num_epochs=8, learning_rate_lyrics=5e-7, learning_rate_coala = 5e-4, save_file='valence_fusion', choi_start_ep=125)\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion_2(test_dataloader,i+1, load_file='valence_fusion')\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain epoch 30\n",
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train lyrics + ConNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, id, label):\n",
    "      self.input_ids = input_ids\n",
    "      self.attention_masks = attention_masks\n",
    "      self.id = id\n",
    "      self.labels = label\n",
    "    def __getitem__(self, index):\n",
    "      input_id = self.input_ids[index]\n",
    "      attention_mask = self.attention_masks[index]\n",
    "      mel = np.load('choi_mel/' + self.id[index] +'.npy') \n",
    "      mel = torch.tensor(mel.reshape(1, 96, 1360))\n",
    "      label = self.labels[index]\n",
    "      return input_id, attention_mask, mel, label\n",
    "    def __len__(self):\n",
    "      return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.lyric = model1\n",
    "        self.convNet = model2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(60,32),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, mel):\n",
    "        lyric_outputs = self.lyric.base_model(input_ids, attention_mask=attention_mask)\n",
    "        convNet_outputs = self.convNet.conv_layers(mel)\n",
    "        convNet_outputs = self.convNet.pool(convNet_outputs)\n",
    "        convNet_outputs = convNet_outputs.squeeze(2).squeeze(2)  \n",
    "        outputs = torch.cat((lyric_outputs[0], convNet_outputs),1)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear(outputs)                  #output:2\n",
    "        '''convNet_outputs = self.convNet.linear2(convNet_outputs) #output:2\n",
    "        lyric_outputs = self.lyric.linear(lyric_outputs) #output:2\n",
    "        outputs = self.alpha * outputs + (1-self.alpha) * convNet_outputs\n",
    "        return convNet_outputs, lyric_outputs, outputs'''\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_3(train_dataloader, val_dataloader,fold, num_epochs = 10, validation = True, save_file = 'valence_fusion_convbert', learning_rate_lyrics = 5e-7, learning_rate_convNet = 5e-5,learning_rate_hybrid = 5e-4, convNet_start_ep = 20):\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    model = ConvNet(*args, conv_until = 5)\n",
    "    pytorch_state_dict = model.state_dict()\n",
    "    '''for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "        # Convert weights from Keras to PyTorch format\n",
    "        if 'weight' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.transpose(convNet_weight[1][0], (3,2,0,1)))\n",
    "        elif 'bias' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][i]))'''\n",
    "    '''pytorch_state_dict = model.state_dict()\n",
    "    count_c = 0\n",
    "    count_b = 0\n",
    "    for i, (name, param) in enumerate(pytorch_state_dict.items()):\n",
    "        # Convert weights from Keras to PyTorch format\n",
    "        if 'weight' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.transpose(convNet_weight[1][0+count_c*6], (3,2,0,1)))\n",
    "            count_c += 1\n",
    "        elif 'bias' in name and 'linear' not in name :\n",
    "            pytorch_state_dict[name] = torch.from_numpy(np.asarray(convNet_weight[1][1+count_b*6]))\n",
    "            count_b += 1\n",
    "    model = HybridModel(PosModel('SamLowe/roberta-base-go_emotions'),model).to(device)'''\n",
    "    model = HybridModel(PosModel('SamLowe/roberta-base-go_emotions'),ConvNet(*args, conv_until = 5)).to(device)\n",
    "    model.convNet.conv_layers = torch.load('valence_convNet/fold_{}/model_epoch_'.format(fold)+str(convNet_start_ep)+'_1.pt')\n",
    "    lyrics_optimizer = torch.optim.AdamW(model.lyric.base_model.parameters(), lr=learning_rate_lyrics)\n",
    "    convNet_optimizer = torch.optim.AdamW(model.convNet.conv_layers.parameters(), lr=learning_rate_convNet)\n",
    "    hybrid_optimizer = torch.optim.AdamW(model.linear.parameters(), lr=learning_rate_hybrid)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #num_epochs = 10  # Set the number of training epochs\n",
    "    alpha = 0.5\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #model.train()\n",
    "        total_loss = 0\n",
    "        train_loss = 0\n",
    "        batch_id = 0\n",
    "        correct = 0\n",
    "        print(f\"Epoch: {epoch + 1}\",'training')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            mel = batch[2].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                mel = mel\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            lyrics_optimizer.zero_grad()\n",
    "            convNet_optimizer.zero_grad()\n",
    "            hybrid_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            lyrics_optimizer.step()\n",
    "            convNet_optimizer.step()\n",
    "            hybrid_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            #print(outputs.logits)\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels).sum()\n",
    "            \n",
    "            print('batch:', batch_id, '/',str(len(train_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "            batch_id += 1\n",
    "            \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Loss: {average_loss:.4f}\", 'accuracy:', correct.item()/len(train_dataloader))\n",
    "        #model.eval()\n",
    "        if validation:\n",
    "            print('validation')\n",
    "            model.eval()\n",
    "            prediction = []\n",
    "            ans = []\n",
    "            val_loss = 0.0\n",
    "            batch_id = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad(): \n",
    "                for batch in val_dataloader:\n",
    "                    input_ids =  batch[0].to(device)\n",
    "                    attention_mask =  batch[1].to(device)\n",
    "                    mel = batch[2].to(device)\n",
    "                    mel = mel.to(torch.float32)\n",
    "                    labels = batch[3].to(device)\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        mel = mel\n",
    "                    )\n",
    "                    loss = loss_function(outputs, labels.to(device))\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _,predict_label = torch.max(outputs,1)\n",
    "                    correct += (predict_label==labels).sum()\n",
    "                    \n",
    "                    prediction.append(predict_label.cpu().item())\n",
    "                    ans.append(labels.cpu().item())\n",
    "                    print('batch:', batch_id, '/',str(len(val_dataloader)), 'loss:', loss.item(), end='\\r')\n",
    "                    batch_id += 1\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_accuracy = correct.item() / len(val_dataloader)\n",
    "            print('loss:', val_loss)\n",
    "            print('accuracy:',val_accuracy)\n",
    "            print('f1 score:', f1_score(ans, prediction, average='macro'))\n",
    "            #if val_monitor == 'loss':\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_best_loss'.format(fold))\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_loss'.format(fold))\n",
    "                torch.save(model.convNet.conv_layers, save_file + '/fold_{}/convNet_best_loss1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/hybrid_model_best_loss'.format(fold) + '.pt')\n",
    "            #else:\n",
    "            if val_accuracy >= best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "                if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                    os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "                model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_best_acc'.format(fold))\n",
    "                tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_best_acc'.format(fold))\n",
    "                torch.save(model.convNet.conv_layers, save_file + '/fold_{}/convNet_best_acc1'.format(fold) + '.pt')\n",
    "                torch.save(model.linear, save_file + '/fold_{}/hybrid_model_best_acc'.format(fold) + '.pt')\n",
    "        else:\n",
    "            if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "            if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "                os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "            model.lyric.save_pretrained(save_file + '/fold_{}/model'.format(fold) + str(epoch+1))\n",
    "            tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer'.format(fold) + str(epoch+1))\n",
    "        if not os.path.isdir(save_file):\n",
    "                    os.mkdir(save_file)\n",
    "        if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "            os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "        model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_epoch_'.format(fold)+ str(epoch+1))\n",
    "        tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_epoch_'.format(fold)+ str(epoch+1))\n",
    "        torch.save(model.convNet.conv_layers, save_file + '/fold_{}/convNet_epoch_'.format(fold)+ str(epoch+1) + '.pt')\n",
    "        torch.save(model.linear, save_file + '/fold_{}/hybrid_model_epoch_'.format(fold)+ str(epoch+1) + '.pt')\n",
    "    if not os.path.isdir(save_file):\n",
    "        os.mkdir(save_file)\n",
    "    if not os.path.isdir(save_file + '/fold_{}'.format(fold)):\n",
    "        os.mkdir(save_file + '/fold_{}'.format(fold))\n",
    "    model.lyric.base_model.save_pretrained(save_file + '/fold_{}/model_final'.format(fold))\n",
    "    tokenizer.save_pretrained(save_file + '/fold_{}/tokenizer_final'.format(fold))\n",
    "    torch.save(model.convNet.conv_layers, save_file + '/fold_{}/convNet_final1'.format(fold) + '.pt')\n",
    "    torch.save(model.linear, save_file + '/fold_{}/hybrid_model_final'.format(fold) + '.pt')\n",
    "    \n",
    "def test_fusion_3(test_dataloader,fold, load_best = 'loss', load_file = 'valence_fusion_convbert'):\n",
    "    print('testing')\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    if load_best == 'loss':\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.convNet.conv_layers = torch.load(load_file + \"/fold_{}/convNet_best_loss1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_loss\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.convNet.conv_layers = torch.load(load_file + \"/fold_{}/convNet_best_acc1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_acc\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        model = HybridModel(PosModel(load_file + \"/fold_{}/model_final\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.convNet.conv_layers = torch.load(load_file + \"/fold_{}/convNet_final1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_final\".format(fold)+ '.pt')\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    #alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            mel = batch[2].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                mel = mel\n",
    "            )\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNet_start_ep = [11,15,10,16,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    if i != 0:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    train_id, val_id, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(np.array(sp_id, dtype=\"object\")[train], np.array(input_ids, dtype=\"object\")[train], np.array(attention_masks, dtype=\"object\")[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, id=train_id, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, id=val_id, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion_3(train_dataloader, val_dataloader,i+1,num_epochs=5, learning_rate_lyrics=5e-7, learning_rate_convNet = 5e-5,learning_rate_hybrid=5e-4, convNet_start_ep=convNet_start_ep[i])\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion_3(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    if i != 4 and i != 2:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    train_id, val_id, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(np.array(sp_id, dtype=\"object\")[train], np.array(input_ids, dtype=\"object\")[train], np.array(attention_masks, dtype=\"object\")[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, id=train_id, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, id=val_id, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion_3(train_dataloader, val_dataloader,i+1,num_epochs=10, learning_rate_lyrics=5e-7, learning_rate_convNet = 5e-5,learning_rate_hybrid=5e-5, convNet_start_ep=convNet_start_ep[i])\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion_3(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    train_id, val_id, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(np.array(sp_id, dtype=\"object\")[train], np.array(input_ids, dtype=\"object\")[train], np.array(attention_masks, dtype=\"object\")[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, id=train_id, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, id=val_id, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion_3(train_dataloader, val_dataloader,i+1,num_epochs=10, learning_rate_lyrics=5e-7, learning_rate_convNet = 1e-5,learning_rate_hybrid=5e-4, convNet_start_ep=convNet_start_ep[i])\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion_3(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fusion_all(test_dataloader,fold, load_best = 'loss', load_file = 'valence_fusion_convbert', epoch = 5):\n",
    "    print('testing')\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    model = HybridModel(PosModel(load_file + \"/fold_{}/model_epoch_\".format(fold)+ str(epoch)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "    model.convNet.conv_layers = torch.load(load_file + \"/fold_{}/convNet_epoch_\".format(fold)+ str(epoch)+ '.pt')\n",
    "    model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_epoch_\".format(fold)+ str(epoch)+ '.pt')\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    #alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            mel = batch[2].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                mel = mel\n",
    "            )\n",
    "            _,predict_label = torch.max(outputs,1)\n",
    "            correct += (predict_label==labels.to(device)).sum()\n",
    "            prediction.append(predict_label.cpu().item())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    accuracy = correct.item() / len(test_dataloader)\n",
    "    f1 = f1_score(ans, prediction, average=None)\n",
    "    prec_recall = precision_recall_fscore_support(ans, prediction)\n",
    "    conf_m = confusion_matrix(prediction,ans)\n",
    "    return accuracy, f1, prec_recall,  conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    if i != 0:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    \n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "      \n",
    "    for j in range(1,6):\n",
    "        print('epoch ', j)\n",
    "        accuracy, f1, prec_recall, confusion_m = test_fusion_all(test_dataloader,i+1, epoch = j)\n",
    "        print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    if i != 4 and i!= 2:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    \n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "      \n",
    "    for j in range(1,11):\n",
    "        print('epoch ', j)\n",
    "        accuracy, f1, prec_recall, confusion_m = test_fusion_all(test_dataloader,i+1, epoch = j)\n",
    "        print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    \n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "      \n",
    "    for j in range(1,11):\n",
    "        print('epoch ', j)\n",
    "        accuracy, f1, prec_recall, confusion_m = test_fusion_all(test_dataloader,i+1, epoch = j)\n",
    "        print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "conf_m = []\n",
    "#test_order_a = []\n",
    "#pred_order_a = []\n",
    "confusion_matrixs = []\n",
    "test_macro_f1 = []\n",
    "test_f1_0 = []\n",
    "test_f1_1 = []\n",
    "test_f1 = []\n",
    "test_precision_0 = []\n",
    "test_recall_0 = []\n",
    "test_precision_1 = []\n",
    "test_recall_1 = []\n",
    "#batch_size = 32\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    \n",
    "    print('Fold {}:'.format(i+1))\n",
    "    train_id, val_id, train_ids, val_ids, train_mask, val_mask, train_label, val_label = train_test_split(np.array(sp_id, dtype=\"object\")[train], np.array(input_ids, dtype=\"object\")[train], np.array(attention_masks, dtype=\"object\")[train], valence[train], test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = TrainDataset(input_ids = train_ids, attention_masks = train_mask, id=train_id, label = train_label)\n",
    "    train_dataloader =  DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    val_dataset = TrainDataset(input_ids = val_ids, attention_masks = val_mask, id=val_id, label = val_label)\n",
    "    val_dataloader =  DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    train_fusion_3(train_dataloader, val_dataloader,i+1,num_epochs=10, learning_rate_lyrics=5e-7, learning_rate_convNet = 5e-4, convNet_start_ep=5)\n",
    "    \n",
    "    accuracy, f1, prec_recall, confusion_m = test_fusion_3(test_dataloader,i+1)\n",
    "    print(\"test accuracy:\",accuracy,\"f1 score:\",f1)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_f1_0.append(f1[0])\n",
    "    test_f1_1.append(f1[1])\n",
    "    test_precision_0.append(prec_recall[0][0])\n",
    "    test_precision_1.append(prec_recall[0][1])\n",
    "    test_recall_0.append(prec_recall[1][0])\n",
    "    test_recall_1.append(prec_recall[1][1])\n",
    "    confusion_matrixs.append(confusion_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf = confusion_matrixs[0]*0\n",
    "for i in range(0,len(test_accuracy)):\n",
    "    print(\"Fold {}:\".format(i+1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrixs[i])\n",
    "    total_conf += confusion_matrixs[i]\n",
    "    print('accuracy:', test_accuracy[i])\n",
    "    print('recall 0:',test_recall_0[i],',recall 1:',test_recall_1[i])\n",
    "    print('precision 0:',test_precision_0[i],',precision 1:',test_precision_1[i])\n",
    "    print('F1 score 0:', test_f1_0[i], 'F1 score 1:', test_f1_1[i])\n",
    "print(\"Total Confusion Matrix:\\n\",total_conf)\n",
    "print(\"Avg accuracy:\",np.array(test_accuracy).mean())\n",
    "print(\"Avg recall 0:\",np.array(test_recall_0).mean(),\",Avg recall 1:\",np.array(test_recall_1).mean())\n",
    "print(\"Avg precision 0:\",np.array(test_precision_0).mean(),\",Avg precision 1:\",np.array(test_precision_1).mean())\n",
    "print(\"Avg f1 score 0:\",np.array(test_f1_0).mean(),\",Avg f1 score 1:\",np.array(test_f1_1).mean())\n",
    "#print(\"Avg f1 score:\",np.array(test_f1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_intermediate_output(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(get_intermediate_output, self).__init__()\n",
    "        self.lyric = model1\n",
    "        self.convNet = model2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(60,32),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, mel):\n",
    "        lyric_outputs = self.lyric.base_model(input_ids, attention_mask=attention_mask)\n",
    "        convNet_outputs = self.convNet.conv_layers(mel)\n",
    "        convNet_outputs = self.convNet.pool(convNet_outputs)\n",
    "        convNet_outputs = convNet_outputs.squeeze(2).squeeze(2)  \n",
    "        outputs = torch.cat((lyric_outputs[0], convNet_outputs),1)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear[0](outputs)                  #output:2\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_output(test_dataloader,fold, load_best = 'loss', load_file = 'valence_fusion', accuracy=804):\n",
    "    print('testing')\n",
    "    '''poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    if load_best == 'loss':\n",
    "        model = get_intermediate_output(PosModel(load_file + \"/fold_{}/model_best_loss\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.lyric.linear = torch.load(load_file + \"/fold_{}/model_best_loss1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_loss\".format(fold)+ '.pt')\n",
    "    elif load_best == 'accuracy':\n",
    "        model = get_intermediate_output(PosModel(load_file + \"/fold_{}/model_best_acc\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.lyric.linear = torch.load(load_file + \"/fold_{}/model_best_acc1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_best_acc\".format(fold)+ '.pt')\n",
    "    else:\n",
    "        model = get_intermediate_output(PosModel(load_file + \"/fold_{}/model_final\".format(fold)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "        model.lyric.linear = torch.load(load_file + \"/fold_{}/model_final1\".format(fold)+ '.pt')\n",
    "        model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_final\".format(fold)+ '.pt')'''\n",
    "    poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    args = [5,#num_conlayer\n",
    "                [32, 32, 32, 32, 32],#num_feat_map\n",
    "                1.0, #feat_scale_factor\n",
    "                [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)], #convSizes\n",
    "                poolings, #pool_sozes\n",
    "                0.0, #dropout_conv\n",
    "                (1,1,96,1360)]#intputshape\n",
    "    model = get_intermediate_output(PosModel(load_file + \"/fold_{}/model_\".format(fold)+ str(accuracy)),ConvNet(*args, conv_until = 5)).to(device)\n",
    "    model.convNet.conv_layers = torch.load(load_file + \"/fold_{}/convNet_\".format(fold)+ str(accuracy)+ '.pt')\n",
    "    model.linear = torch.load(load_file + \"/fold_{}/hybrid_model_\".format(fold)+ str(accuracy)+ '.pt')\n",
    "    \n",
    "    model.eval()\n",
    "    layer_output = []\n",
    "    ans = []\n",
    "    batch_id = 0\n",
    "    correct = 0\n",
    "    #alpha = 0.5\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            input_ids =  batch[0].to(device)\n",
    "            attention_mask =  batch[1].to(device)\n",
    "            mel = batch[2].to(device)\n",
    "            mel = mel.to(torch.float32)\n",
    "            labels = batch[3].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                mel = mel\n",
    "            )\n",
    "            layer_output.append(outputs[0].cpu())\n",
    "            ans.append(labels.cpu().item())\n",
    "            batch_id += 1\n",
    "            print('batch:', batch_id, end='\\r')\n",
    "    \n",
    "    return layer_output, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train, test) in tqdm(enumerate(kfold.split(input_ids, choi_rep, valence))):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    print('Fold {}:'.format(i+1))\n",
    "    test_dataset = TrainDataset(input_ids = np.array(input_ids, dtype=\"object\")[test], attention_masks = np.array(attention_masks, dtype=\"object\")[test], id=np.array(sp_id, dtype=\"object\")[test], label = valence[test])\n",
    "    test_dataloader =  DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    \n",
    "    layer_output, ans = intermediate_output(test_dataloader,i+1, load_file='valence_fusion_convbert_best', accuracy=823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplayer_output = [t.numpy() for t in layer_output[0:1000]]\n",
    "nplayer_output = np.asarray(nplayer_output)\n",
    "nplayer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_conv_tsne = TSNE(n_components=2, init='random', random_state=5, verbose=1).fit_transform(nplayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = ans[0:1000]\n",
    "df[\"comp-1\"] = lyr_conv_tsne[:,0]\n",
    "df[\"comp-2\"] = lyr_conv_tsne[:,1]\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df).set(title=\"BERT_convNet T-SNE projection\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
